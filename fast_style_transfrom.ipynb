{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import tqdm"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2023-05-09T23:01:12.140396Z",
     "iopub.execute_input": "2023-05-09T23:01:12.141320Z",
     "iopub.status.idle": "2023-05-09T23:01:17.315992Z",
     "shell.execute_reply.started": "2023-05-09T23:01:12.141279Z",
     "shell.execute_reply": "2023-05-09T23:01:17.314803Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "in this project, we just use part of the coco dataset, which is the test set"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:01:21.756734Z",
     "iopub.execute_input": "2023-05-09T23:01:21.757376Z",
     "iopub.status.idle": "2023-05-09T23:01:21.793770Z",
     "shell.execute_reply.started": "2023-05-09T23:01:21.757341Z",
     "shell.execute_reply": "2023-05-09T23:01:21.792577Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "cuda\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class MS_COCO(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (str): Path to the directory containing images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.image_paths = [os.path.join(img_dir, img_name) for img_name in os.listdir(img_dir)]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "img_size = [256, 256]\n",
    "# define the transform\n",
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "transforms = torchvision.transforms.Compose([\n",
    "             torchvision.transforms.Resize(img_size),\n",
    "             torchvision.transforms.ToTensor(),\n",
    "             torchvision.transforms.Normalize(mean=rgb_mean,\n",
    "                                         std=rgb_std)\n",
    "                ])\n",
    "\n",
    "dataset = MS_COCO(\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/test2014\", transform=transforms)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:31:59.536605Z",
     "iopub.execute_input": "2023-05-09T23:31:59.537341Z",
     "iopub.status.idle": "2023-05-09T23:31:59.650092Z",
     "shell.execute_reply.started": "2023-05-09T23:31:59.537299Z",
     "shell.execute_reply": "2023-05-09T23:31:59.648954Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pretrained_net = torchvision.models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:32:00.203447Z",
     "iopub.execute_input": "2023-05-09T23:32:00.204586Z",
     "iopub.status.idle": "2023-05-09T23:32:02.121473Z",
     "shell.execute_reply.started": "2023-05-09T23:32:00.204518Z",
     "shell.execute_reply": "2023-05-09T23:32:02.120457Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# then we need to modified the model\n",
    "net = nn.Sequential(*[pretrained_net.features[i] for i in range(24)])\n",
    "net = net.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:32:02.811267Z",
     "iopub.execute_input": "2023-05-09T23:32:02.811716Z",
     "iopub.status.idle": "2023-05-09T23:32:02.828410Z",
     "shell.execute_reply.started": "2023-05-09T23:32:02.811670Z",
     "shell.execute_reply": "2023-05-09T23:32:02.827436Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# we define the model for \"generate\" an image\n",
    "def ConvLayer(in_channels, out_channels, kernel_size=3, stride=1, \n",
    "    upsample=None, instance_norm=True, relu=True):\n",
    "    layers = []\n",
    "    if upsample:\n",
    "        layers.append(nn.Upsample(mode='nearest', scale_factor=upsample))\n",
    "    layers.append(nn.ReflectionPad2d(kernel_size // 2))\n",
    "    layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride))\n",
    "    if instance_norm:\n",
    "        layers.append(nn.InstanceNorm2d(out_channels))\n",
    "    if relu:\n",
    "        layers.append(nn.ReLU())\n",
    "    return layers\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            *ConvLayer(channels, channels, kernel_size=3, stride=1), \n",
    "            *ConvLayer(channels, channels, kernel_size=3, stride=1, relu=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + x\n",
    "\n",
    "class TransformNet(nn.Module):\n",
    "    def __init__(self, base=32):\n",
    "        super(TransformNet, self).__init__()\n",
    "        self.downsampling = nn.Sequential(\n",
    "            *ConvLayer(3, base, kernel_size=9), \n",
    "            *ConvLayer(base, base*2, kernel_size=3, stride=2), \n",
    "            *ConvLayer(base*2, base*4, kernel_size=3, stride=2), \n",
    "        )\n",
    "        self.residuals = nn.Sequential(*[ResidualBlock(base*4) for i in range(5)])\n",
    "        self.upsampling = nn.Sequential(\n",
    "            *ConvLayer(base*4, base*2, kernel_size=3, upsample=2),\n",
    "            *ConvLayer(base*2, base, kernel_size=3, upsample=2),\n",
    "            *ConvLayer(base, 3, kernel_size=9, instance_norm=False, relu=False),\n",
    "        )\n",
    "    \n",
    "    def forward(self, X):\n",
    "        y = self.downsampling(X)\n",
    "        y = self.residuals(y)\n",
    "        y = self.upsampling(y)\n",
    "        return y"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:32:03.480831Z",
     "iopub.execute_input": "2023-05-09T23:32:03.481199Z",
     "iopub.status.idle": "2023-05-09T23:32:03.495713Z",
     "shell.execute_reply.started": "2023-05-09T23:32:03.481169Z",
     "shell.execute_reply": "2023-05-09T23:32:03.494239Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tf_net = TransformNet()\n",
    "tf_net = tf_net.to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:32:04.126458Z",
     "iopub.execute_input": "2023-05-09T23:32:04.127405Z",
     "iopub.status.idle": "2023-05-09T23:32:04.162865Z",
     "shell.execute_reply.started": "2023-05-09T23:32:04.127357Z",
     "shell.execute_reply": "2023-05-09T23:32:04.161777Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "handle the style image"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "style_path = '/kaggle/input/style-image/the_scream.jpg'\n",
    "style_image = Image.open(style_path).convert('RGB')\n",
    "style_tensor = transforms(style_image)\n",
    "style_tensor = style_tensor.unsqueeze(0)\n",
    "style_tensor = style_tensor.to(device)\n",
    "# style_image"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:32:05.206491Z",
     "iopub.execute_input": "2023-05-09T23:32:05.207106Z",
     "iopub.status.idle": "2023-05-09T23:32:05.245046Z",
     "shell.execute_reply.started": "2023-05-09T23:32:05.207070Z",
     "shell.execute_reply": "2023-05-09T23:32:05.243931Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract content and style"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_feature(images, vgg_model, content_layer, style_layers):\n",
    "    styles = []\n",
    "    x = images\n",
    "    for i in range(len(vgg_model)):\n",
    "        x = vgg_model[i](x)\n",
    "        if i in style_layers:\n",
    "            styles.append(x)\n",
    "        if i in content_layer:\n",
    "            content = x\n",
    "    return content, styles"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:32:06.237578Z",
     "iopub.execute_input": "2023-05-09T23:32:06.238636Z",
     "iopub.status.idle": "2023-05-09T23:32:06.245642Z",
     "shell.execute_reply.started": "2023-05-09T23:32:06.238589Z",
     "shell.execute_reply": "2023-05-09T23:32:06.244203Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate loss\n",
    "#### Calculate content loss\n",
    "#### Calculate style loss\n",
    "#### Calculate TV loss"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# content_loss_fn = nn.MSELoss()\n",
    "# content_weight = 1\n",
    "# content_loss = content_weight * content_loss_fn(real_content_out.repeat(fake_content_out.size(0),1,1,1), \n",
    "#                                                 fake_content_out)\n",
    "def calculate_content_loss(content_weight, content_loss_fn, \n",
    "                           transformed_content, original_content):\n",
    "    \"\"\"\n",
    "    # since we just select one layer as the content layer, so no need to consider the loops\n",
    "    content_weight:\n",
    "    content_loss_fn: MSELoss\n",
    "    transformed_content: the output of content layer of 'fake' image, torch.Tensor\n",
    "    original_content: the output of content layer of style image, torch.Tensor\n",
    "    \"\"\"\n",
    "    content_loss = content_weight * content_loss_fn(original_content, transformed_content)\n",
    "    return content_loss\n",
    "\n",
    "def calculate_tv_loss(tv_weight, transfromed_img):\n",
    "    \"\"\"\n",
    "    tv_weight:\n",
    "    transformed_img: output from transform network\n",
    "    \"\"\"\n",
    "    return tv_weight * (torch.abs(transfromed_img[:, :, 1:, :] - transfromed_img[:, :, :-1, :]).mean() +\n",
    "                        torch.abs(transfromed_img[:, :, :, 1:] - transfromed_img[:, :, :, :-1]).mean())\n",
    "\n",
    "def gram_matrix(y):\n",
    "    (b, ch, h, w) = y.size()\n",
    "    features = y.view(b, ch, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    gram = features.bmm(features_t) / (ch * h * w)\n",
    "    return gram\n",
    "\n",
    "def calculate_style_loss(style_weight, style_loss_fn, \n",
    "                         transformed_style, original_style_gram):\n",
    "    \"\"\"\n",
    "    # the style layers is not a single layer, so we need to consider loop\n",
    "    style_weight: \n",
    "    sytle_loss_fn: MSELoss\n",
    "    transformed_style: the outputs of sytle layers of 'fake' image, list[torch.Tensor]\n",
    "    original_style_gram: the gram matrix outputs of sytle layers of original image, list[torch.Tensor]\n",
    "    Note:\n",
    "        since this project is for a fixed style transform, therefore we don't calcualte the style and gram of target style while training.\n",
    "        in order to save some computational resource\n",
    "    \"\"\"\n",
    "    style_loss = 0\n",
    "    transformed_grams = [gram_matrix(x) for x in transformed_style]\n",
    "    for transformed_gram, original_style_gram in zip(transformed_grams, original_style_gram):\n",
    "        original_style_gram = original_style_gram.detach()\n",
    "        style_loss += style_weight * style_loss_fn(transformed_gram, \n",
    "                                                   original_style_gram.expand_as(transformed_gram))\n",
    "    return style_loss"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:32:07.143903Z",
     "iopub.execute_input": "2023-05-09T23:32:07.145064Z",
     "iopub.status.idle": "2023-05-09T23:32:07.157586Z",
     "shell.execute_reply.started": "2023-05-09T23:32:07.145017Z",
     "shell.execute_reply": "2023-05-09T23:32:07.156388Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# train:\n",
    "# now we have all the components, then we just need put them togather\n",
    "# test one epoch\n",
    "\n",
    "content_layer = [23]\n",
    "style_layers = [4, 9, 16, 23]\n",
    "lr = 0.01\n",
    "epochs = 5\n",
    "\n",
    "optimizer = torch.optim.Adam(tf_net.parameters(), lr=lr)\n",
    "# so first we need to get the style gram matrix of the original style image\n",
    "_, target_style = extract_feature(style_tensor, net, content_layer, style_layers)\n",
    "# target_style = target_style.to(device)\n",
    "# calcualte target gram matrix\n",
    "target_gram = [gram_matrix(x).to(device) for x in target_style]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    content_losses, style_losses, Tv_losses = [], [], []\n",
    "#     with tqdm(enumerate(data_loader), total=n_batch) as pbar\n",
    "    for i, content_img in enumerate(tqdm.tqdm(dataloader)):\n",
    "        # keep track the losses\n",
    "        c_losses, s_losses, tv_losses = [], [], []\n",
    "\n",
    "        # clear the grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # move to device\n",
    "        content_img = content_img.to(device)\n",
    "\n",
    "        # first we get the transformed image\n",
    "        transformed_img = tf_net(content_img)\n",
    "\n",
    "        # then we need to get the content of content_image\n",
    "        target_content, _ = extract_feature(content_img, net, content_layer, style_layers)\n",
    "        # content and style of the transformed image\n",
    "        transformed_content, transformed_styles = extract_feature(transformed_img, net, content_layer, style_layers) \n",
    "\n",
    "        # then we need to calcualte the content loss\n",
    "        c_loss = calculate_content_loss(1, nn.MSELoss(), transformed_content, target_content)\n",
    "        c_losses.append(c_loss.item())\n",
    "\n",
    "        # calculate the style loss\n",
    "        s_loss = calculate_style_loss(1e3, nn.MSELoss(), transformed_styles, target_gram)\n",
    "        s_losses.append(s_loss.item())\n",
    "\n",
    "        # calculate the tv loss\n",
    "        tv_loss = calculate_tv_loss(1, transformed_img)\n",
    "        tv_losses.append(tv_loss.item)\n",
    "\n",
    "        loss = c_loss + s_loss + tv_loss\n",
    "        # then we need to update the parameter of the tf_net\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(sum(c_losses))\n",
    "    print(sum(s_losses))\n",
    "    print(sum(tv_losses))\n",
    "    content_losses.append(sum(c_losses))\n",
    "    style_losses.append(sum(s_losses))\n",
    "    Tv_losses.append(sum(tv_losses))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-05-09T23:53:16.065718Z",
     "iopub.execute_input": "2023-05-09T23:53:16.066090Z"
    },
    "trusted": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "text": " 16%|█▌        | 402/2549 [04:29<24:01,  1.49it/s]",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}